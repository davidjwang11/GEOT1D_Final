{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script>\n",
       "    var toggleCode= function() {\n",
       "        var ins = document.getElementsByClassName(\"CodeMirror\");\n",
       "        for (var i=0; i < ins.length; i++) {\n",
       "            ins[i].style.display= ins[i].style.display === \"none\" ? \"block\": \"none\";\n",
       "        }\n",
       "    };\n",
       "</script>\n",
       "<button onclick=toggleCode()>Click Here to Display/Hide Code</button>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<script>\n",
    "    var toggleCode= function() {\n",
    "        var ins = document.getElementsByClassName(\"CodeMirror\");\n",
    "        for (var i=0; i < ins.length; i++) {\n",
    "            ins[i].style.display= ins[i].style.display === \"none\" ? \"block\": \"none\";\n",
    "        }\n",
    "    };\n",
    "</script>\n",
    "<button onclick=toggleCode()>Click Here to Display/Hide Code</button>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Type 1 Diabetes in Australians - A Geographical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "-----------------\n",
    "\n",
    "This project (GeoT1D) will aim to analyse data from the AURIN database in conjunction with Type 1 Diabetes data from ADDN and ENDIA, looking for correlations and factors which may influence the prevalence of Type 1 Diabetes in Australia.\n",
    "\n",
    "GeoT1D continues the work of Zhanyi Qi, who started her project in early 2017, creating a Jupyter Notebook platform to develop a data visualization and analysis tool for researchers, combining data from the Australian Diabetes Data Network (ADDN), Environmental Determinants of Islet Autoimmunity (ENDIA), and Australian Urban Research Infrastructure Network (AURIN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "--------------------\n",
    "\n",
    ">**The data is gathered in the following ways**\n",
    "\n",
    "> - Manually downloaded from the AURIN portal as zip files and uploaded into the folder AURIN_Datasets\n",
    "\n",
    "> - Loaded directly from the AURIN API (however limited datasets are available through this method)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Do you want to load data from a dataset downloaded from the AURIN Portal(0) or search the AURIN API(1)?0\n",
      "You selected:  0\n",
      "Please ensure that you have downloaded a dataset from the AURIN portal, refer to the instructions below on how to do this\n",
      "Press enter to continue\n"
     ]
    }
   ],
   "source": [
    "PORTAL = \"0\"\n",
    "API = \"1\"\n",
    "\n",
    "select = input(\"Do you want to load data from a dataset downloaded from the AURIN Portal(0) or search the AURIN API(1)?\")\n",
    "print(\"You selected: \", select)\n",
    "\n",
    "if select == PORTAL:\n",
    "    print(\"Please ensure that you have downloaded a dataset from the AURIN portal, refer to the instructions below on how to do this\")\n",
    "    input(\"Press enter to continue\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to download data from the AURIN Portal\n",
    "--------------------\n",
    "\n",
    "> - Go to portal.aurin.org.au\n",
    "> - Login with your education account\n",
    "> - click on add datasets on the right hand side panel\n",
    "> - serach and add a dataset of interest\n",
    "> - back on the right hand side panel download the dataset as a csv\n",
    "> - upload the zipfile downloaded into your desired folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The following cells are a series of functions for data loading\n",
    "-----------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data from downloaded zip files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first offer the user the opportunity to extract a zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Unzip the files from the folder and load them into data structures\n",
    "import os\n",
    "import zipfile\n",
    "import json\n",
    "from csv import DictReader\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "\n",
    "def loadAvailableDatasets(path):\n",
    "    \"\"\"This function will load the datasets in the AURIN_Datasets folder, returns the extracted datasets as well as the non-extracted\"\"\"\n",
    "    f = []\n",
    "    d = []\n",
    "    # go through the folder to find datasets\n",
    "    for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "        f.extend(filenames)\n",
    "        d.extend(dirnames)\n",
    "        break\n",
    "        \n",
    "    extracted = []\n",
    "    unextracted = []\n",
    "    \n",
    "    #all unextract files\n",
    "    for file in f:\n",
    "        if file.endswith('.zip'):\n",
    "            unextracted.append(file)\n",
    "    #extracted files\n",
    "    for directory in d:\n",
    "        if (directory + \".zip\") in unextracted:\n",
    "            extracted.append(directory)\n",
    "            #remove the corresponding one from unextracted\n",
    "            unextracted.remove(directory + \".zip\")\n",
    "\n",
    "    \n",
    "    return extracted, unextracted    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extractFiles(filename, path):\n",
    "    \"\"\"This function will extract a zipfile and create another folder with its contents\"\"\"\n",
    "    \n",
    "    # check to see if the extracted folder already exists \n",
    "    d = []\n",
    "    for (dirpath, dirnames, filenames) in os.walk(path):\n",
    "        d.extend(dirnames)\n",
    "        break\n",
    "    if filename[0:-4] in d:\n",
    "        print(\"File already extracted!\")\n",
    "        return None\n",
    "    \n",
    "    # We will have a new folder to extract the files into with the same name\n",
    "    new_path = path+\"/\"+filename[:-4]\n",
    "    os.makedirs(new_path)\n",
    "    \n",
    "    # extract the file into the folder\n",
    "    with zipfile.ZipFile(path + \"/\" + filename,\"r\") as zip_ref:\n",
    "        zip_ref.extractall(new_path)\n",
    "    print(\"===========================================\")\n",
    "    print(filename + \" extracted successfully...\")\n",
    "    print(new_path + \" created...\")\n",
    "    print(\"===========================================\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After file extraction, the user should select the dataset they want\n",
    "\n",
    "If the dataset is geographically aggregated, the user may then choose to select an attribute or attributes from the dataset for analysis. If an aggregation can't be found, the user is required to manually enter the geographical classication as well as identify the geographical property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadAttributes(folder):\n",
    "    \"\"\"This function will return a list of attributes from the metadata file of a dataset folder\"\"\"\n",
    "    f = []\n",
    "    \n",
    "    #find the json file which has the attributes\n",
    "    for (dirpath, dirnames, filenames) in os.walk(folder):\n",
    "        f.extend(filenames)\n",
    "        break\n",
    "    json_file = \"\"\n",
    "    for file in f:\n",
    "        if file.endswith(\".json\"):\n",
    "            json_file = file\n",
    "    \n",
    "    # open the json file and load the attributes\n",
    "    with open(folder+\"/\"+json_file) as data_file:    \n",
    "        data = json.load(data_file)\n",
    "        \n",
    "    # {Name: description}\n",
    "    attributes = []\n",
    "    for attribute in data[\"selectedAttributes\"]:\n",
    "        attributes.append(attribute)\n",
    "    return attributes\n",
    "\n",
    "def selectAttributes(folder):\n",
    "    \"\"\"For a given folder this function will allow the user to select the relevant attributes from a dataset that they want to load\"\"\"\n",
    "    \n",
    "    attributes = loadAttributes(folder)\n",
    "    count = 0\n",
    "    \n",
    "    #print attributes to user\n",
    "    for a in attributes:\n",
    "        print (\"Attribute {}: \".format(count) + a['title'])\n",
    "        count += 1\n",
    "       \n",
    "    #allow the user to select by index\n",
    "    user_select = input(\"Enter the attribute number you would like to use: (For all attributes, leave blank)\")\n",
    "    if not user_select:\n",
    "        return False\n",
    "    return attributes[int(user_select)]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadData(folder, attributes=False):\n",
    "    \"\"\"For a given dataset folder, this function will return a list which contains the dataset's information of the selected attributes\"\"\"\n",
    "    \n",
    "    # This is the format that the data will be loaded into\n",
    "    # {attribute: [data, data, data, data]}\n",
    "    data = defaultdict(list)\n",
    "    metadata = {}\n",
    "    f = []\n",
    "    \n",
    "    # find the csv file which contains the data\n",
    "    for (dirpath, dirnames, filenames) in os.walk(folder):\n",
    "        f.extend(filenames)\n",
    "        break\n",
    "    csv_file = folder + \"/\"\n",
    "    for file in f:\n",
    "        if file.endswith(\".csv\"):\n",
    "            csv_file += file\n",
    "    \n",
    "    #load attributes\n",
    "    if not attributes:\n",
    "        attributes = loadAttributes(folder)\n",
    "        \n",
    "    for attribute in attributes:\n",
    "        metadata[attribute['name']] = attribute\n",
    "    \n",
    "    #load the data of the attributes\n",
    "    with open(csv_file) as fp:\n",
    "        reader = DictReader(fp, delimiter=',')\n",
    "        for line in reader:\n",
    "            stripped_line = {}\n",
    "            for key, value in dict(line).items():\n",
    "\n",
    "                stripped_line[key.strip()] = value\n",
    "                    \n",
    "            for attribute in attributes:\n",
    "              \n",
    "                value = stripped_line[attribute['name']]\n",
    "                \n",
    "                data[attribute['name']].append(value)\n",
    "                metadata[attribute['name']] = attribute\n",
    "                \n",
    "    #return nicely formatted data\n",
    "    return data, metadata\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently we are only checking for whether the following aggregations as these are the most common and relevant in datasets\n",
    "\n",
    "> - **LGA **\n",
    "> - **SLA**\n",
    "> - **Postcode**\n",
    "> - **sa1**\n",
    "> - **sa2**\n",
    "> - **sa3**\n",
    "> - **sa4 **\n",
    "\n",
    "> - **coordinates (longitude and latitude)** \n",
    ">> - Note: While we are identifying datasets with coordinates, these datasets are not analysed as only a few of them have relevance to T1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Check for Aggregation \n",
    "#Ensure that the dataset is aggregated in one of the aggregations that we are able to analyse\n",
    "#Find the aggregation property\n",
    "def checkAggr(prop):\n",
    "    \"\"\"This function will check a name of a property to see if it suggests that it's geogrpahically classified\"\"\"\n",
    "    \n",
    "    if ((\"lga\" in prop.lower()) and (\"main\" in prop.lower())) or ((\"lga\" in prop.lower()) and (\"code\" in prop.lower())):\n",
    "        return \"lga\"\n",
    "    elif (\"sla\" in prop.lower()) and (\"code\" in prop.lower()) or ((\"sla\" in prop.lower()) and (\"main\" in prop.lower())):\n",
    "        return \"sla\"\n",
    "    elif (\"post\" in prop.lower()) and (\"code\" in prop.lower()):\n",
    "        return  \"postcode\"\n",
    "    elif (\"sa1\" in prop.lower()) and (\"code\" in prop.lower()) or ((\"sa1\" in prop.lower()) and (\"main\" in prop.lower())):\n",
    "        return  \"sa1\"\n",
    "    elif ((\"sa2\" in prop.lower()) and (\"code\" in prop.lower())) or ((\"sa2\" in prop.lower()) and (\"main\" in prop.lower())):\n",
    "        return  \"sa2\"\n",
    "    elif (\"sa3\" in prop.lower()) and (\"code\" in prop.lower())  or ((\"sa3\" in prop.lower()) and (\"main\" in prop.lower())):\n",
    "        return  \"sa3\"\n",
    "    elif (\"sa4\" in prop.lower()) and (\"code\" in prop.lower()) or ((\"sa4\" in prop.lower()) and (\"main\" in prop.lower())):\n",
    "        return  'sa4'\n",
    "    elif (\"long\" in prop.lower()):\n",
    "        return \"longitude\"\n",
    "    elif (\"lat\" in prop.lower()):\n",
    "        return \"latitude\"\n",
    "    return False\n",
    "\n",
    "#currently the code also checks for whether the dataset contains coordinates however, this has yet to be implemented in analysis\n",
    "def checkIfCoordinates(var1, var2):\n",
    "    \"\"\"This function checks whether the two variables are coordinates\"\"\"\n",
    "    \n",
    "    if checkAggr(var1) == \"longitude\":\n",
    "        if checkAggr(var2) == \"latitude\":\n",
    "            return True\n",
    "    elif checkAggr(var1) == \"latitude\":\n",
    "        if checkAggr(var2) == \"longitude\":\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "def findAggr(data, fromAPI=False):\n",
    "    \"\"\"This function will check if a dataset from the API, or alternatively a zip file \n",
    "    has some kind of geographical classification.\n",
    "    \n",
    "    It will return all of the geographical classifications that it has in a list\n",
    "    \n",
    "    It's up to the user to ensure that the correct aggregation is selected\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    geo_properties = []\n",
    "    geo_classifications = []\n",
    "    previous_prop = None\n",
    "    \n",
    "    \n",
    "    if fromAPI:\n",
    "        root = data\n",
    "        \n",
    "        # manually load each property and check if they are aggregations\n",
    "        for element in root.find(\".//xsd:sequence\", root.nsmap):\n",
    "            property_name = element.get('name')            \n",
    "            aggr = checkAggr(property_name)\n",
    "            if aggr and aggr != \"longitude\" and aggr != \"latitude\":\n",
    "                geo_properties.append(property_name)\n",
    "                geo_classifications.append(aggr)\n",
    "            elif aggr and checkIfCoordinates(previous_prop, property_name):\n",
    "                geo_properties.append(previous_prop),\n",
    "                geo_properties.append(property_name),\n",
    "                geo_classifications.append(\"coordinates\")\n",
    "            previous_prop = property_name          \n",
    "    \n",
    "    # not from API\n",
    "    else:\n",
    "        for property_name in data.keys():\n",
    "\n",
    "            aggr = checkAggr(property_name)\n",
    "            if aggr and aggr != \"longitude\" and aggr != \"latitude\":\n",
    "                geo_properties.append(property_name)\n",
    "                geo_classifications.append(aggr)\n",
    "            elif aggr and checkIfCoordinates(previous_prop, property_name):\n",
    "                geo_properties.append(previous_prop)\n",
    "                geo_properties.append(property_name)\n",
    "                geo_classifications.append(\"coordinates\")\n",
    "            previous_prop = property_name\n",
    "    if geo_classifications == []:\n",
    "        return False, False\n",
    "            \n",
    "    return geo_properties, geo_classifications\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is the main function allowing users to choose their dataset and attributes\n",
    "\n",
    "def chooseAttributes():\n",
    "    \n",
    "    \"\"\"This function executes a series of other functions to allow the user to select an attributes from a dataset \n",
    "    for analysis\"\"\"\n",
    "    \n",
    "    #path = input(\"Please enter the path of your datasets: \")\n",
    "    path = \"./AURIN_Datasets\"\n",
    "    print(\"You've entered path: \" + path)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Load the path specified and find what datasets are extracted and what datasets have not yet been extracted\n",
    "    available = loadAvailableDatasets(path)\n",
    "    \n",
    "    def print_datasets(available):\n",
    "        print(\"Extracted datasets: \")\n",
    "        counter = 0\n",
    "        for dataset in available[0]:\n",
    "            print(counter, \"-----\", dataset)\n",
    "            counter += 1 \n",
    "\n",
    "        counter = 0\n",
    "        print(\"Unextracted datasets: \")\n",
    "        for dataset in available[1]:\n",
    "            print(counter, \"-----\", dataset)\n",
    "            counter += 1 \n",
    "\n",
    "    \n",
    "    print_datasets(available)\n",
    "    \n",
    "    extract = input(\"Do you want to extract a zipfile? Y/N \")\n",
    "    if extract == \"Y\" or extract == \"y\":\n",
    "        file_extract = input(\"What file do you want to extract? (Enter index from Unextracted) \")\n",
    "        extractFiles(available[1][int(file_extract)], path)\n",
    "        available = loadAvailableDatasets(path)\n",
    "        print_datasets(available)\n",
    "        \n",
    "    index = input(\"Please enter the name of the dataset you want to analyse (Enter index from Extracted)\")\n",
    "    \n",
    "    selected_dataset = path+\"/\"+available[0][int(index)]\n",
    "    \n",
    "    geo_properties, geo_classifications = findAggr(loadData(selected_dataset)[0])\n",
    "    print(\"The Geographical Classifications of the dataset are\", geo_classifications)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if geo_classifications == False:\n",
    "        print(\"===========================================================================================\")\n",
    "        print(\"Cannot find an aggregation/geographical level for the dataset\")\n",
    "        print(\"===========================================================================================\")\n",
    "        \n",
    "        # the user has to manually tell the program what geographical property is the aggregation (if any) \n",
    "        # and also declaring the classification of the property\n",
    "        for a in loadAttributes(selected_dataset):\n",
    "            print (a[\"name\"])\n",
    "        geo_properties = input(\"Please manually enter the geographical property of the dataset\")\n",
    "        geo_classifications = input(\"Please manually enter the geographical classification of the dataset\")\n",
    "        geo_properties = [geo_properties]\n",
    "        geo_classifications = [geo_classifications]\n",
    "    \n",
    "    \n",
    "    selected_attributes = []\n",
    "   \n",
    "    \n",
    "    # select attributes for analysis\n",
    "    while True:\n",
    "        selected = selectAttributes(selected_dataset)\n",
    "        \n",
    "        #The user wants all attributes\n",
    "        if not selected:\n",
    "            selected_attributes = False\n",
    "            print(\"Nothing selected\")\n",
    "            break\n",
    "        else:\n",
    "            selected_attributes.append(selected)\n",
    "            again = input(\"Do you want to select another attribute? Y/N \")\n",
    "            if again == \"N\" or again == \"n\":\n",
    "                break\n",
    "    print(\"You selected: \", selected_attributes)\n",
    "    \n",
    "    # Allow the user to select the geo_classification\n",
    "    print(\"The geographical aggregation of the dataset is: \", geo_classifications)\n",
    "    key_index = input('Please input a valid aggregation/geogrpahical classification level code (Index): ')\n",
    "    chosen_classification = geo_classifications[int(key_index)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    aurin_props = selected_attributes\n",
    "    \n",
    "    \n",
    "    for prop in geo_properties:\n",
    "        selected_attributes.append({\"name\": prop})\n",
    "\n",
    "\n",
    "    print(\"selected attributes are: \", selected_attributes)\n",
    "    print(\"geo properties are: \", geo_properties)\n",
    "\n",
    "    data, metadata = loadData(selected_dataset, selected_attributes)\n",
    "    \n",
    "    \n",
    "\n",
    "    return data, metadata, chosen_classification, geo_properties, aurin_props\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Data manually from the AURIN Portal \n",
    "\n",
    "### Allow the user to choose the dataset to be analysed\n",
    "\n",
    "Load the data from the dataset accordingly from user selected attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You've entered path: ./AURIN_Datasets\n",
      "Extracted datasets: \n",
      "0 ----- LGA-G13a_Language_Spoken_at_Home_by_Proficiency_in_Spoken_Eng_by_Sex-Census_2016.csv\n",
      "1 ----- SA2_Non-English_speaking_countries_of_birth.csv\n",
      "2 ----- SA4-G02_Selected_Medians_and_Averages-Census_2016.csv\n",
      "3 ----- Local_Government_Area__LGA__profiles_data_2015_for_VIC.csv\n",
      "4 ----- LGA15_Mothers__smoking__and_Babies_-_2012-2015.csv\n",
      "5 ----- LGA15_Adults_Health_Risk_Factor_Estimates_-_2014-2015.csv\n",
      "6 ----- SLA11_Premature_Mortality.csv\n",
      "7 ----- LGA11-based_B02_Selected_Medians_and_Averages_as_at_2011-08-11.csv\n",
      "8 ----- LGA_Real_Income_per_Taxpayer__RIPT__1980-81_to_2005-06_for_Australia.csv\n",
      "9 ----- LGA_Sedentary_behaviour__sitting_hours_per_day_.csv\n",
      "10 ----- LGA_Adequate_work-life_balance.csv\n",
      "11 ----- SLA_Long_commute__2_hours_per_day_.csv\n",
      "12 ----- SLA_Sedentary_behaviour__sitting_hours_per_day_.csv\n",
      "13 ----- LGA_Visit_to_green_space___once_per_week_.csv\n",
      "14 ----- SA2_National_Regional_Profile__NRP__-_People_Population_2009_-_2013.csv\n",
      "15 ----- SLA_Adequate_work-life_balance.csv\n",
      "Unextracted datasets: \n",
      "0 ----- LGA_Visit_to_green_space___once_per_week_.csv (1).zip\n",
      "1 ----- LGA-G19_Voluntary_Work_by_Age_by_Sex-Census_2016.csv.zip\n",
      "2 ----- LGA_Household_Travel_Survey_2011.csv.zip\n",
      "3 ----- SLA_Number_of_Non-Taxables_1990-91_to_2005-06_for_Australia.csv.zip\n",
      "4 ----- Local_Government_Area__LGA__profiles_data_2011.csv.zip\n",
      "5 ----- LGA_7_day___spend_at_a_licensed_premises__of_those_purchasing_.csv.zip\n",
      "Do you want to extract a zipfile? Y/N n\n",
      "Please enter the name of the dataset you want to analyse (Enter index from Extracted)1\n",
      "The Geographical Classifications of the dataset are False\n",
      "===========================================================================================\n",
      "Cannot find an aggregation/geographical level for the dataset\n",
      "===========================================================================================\n",
      "area_code\n",
      "area_name\n",
      "ctry_vietnm_2_denom_6_11_6_11\n",
      "ctry_vietnm_3_percent_6_11_6_11\n",
      "ctry_lebnon_1_no_6_11_6_11\n",
      "ctry_china_3_percent_6_11_6_11\n",
      "ctry_germny_1_no_6_11_6_11\n",
      "ctry_greece_3_percent_6_11_6_11\n",
      "ctry_germny_3_percent_6_11_6_11\n",
      "ctry_greece_1_no_6_11_6_11\n",
      "ctry_india_1_no_6_11_6_11\n",
      "ctry_india_3_percent_6_11_6_11\n",
      "ctry_italy_1_no_6_11_6_11\n",
      "ctry_italy_3_percent_6_11_6_11\n",
      "ctry_lebnon_3_percent_6_11_6_11\n",
      "ctry_malay_1_no_6_11_6_11\n",
      "ctry_phillp_3_percent_6_11_6_11\n",
      "ctry_malay_3_percent_6_11_6_11\n",
      "ctry_phillp_1_no_6_11_6_11\n",
      "ctry_srilnk_1_no_6_11_6_11\n",
      "ctry_srilnk_3_percent_6_11_6_11\n",
      "ctry_china_1_no_6_11_6_11\n",
      "ctry_vietnm_1_no_6_11_6_11\n",
      "Please manually enter the geographical property of the datasetarea_code\n",
      "Please manually enter the geographical classification of the datasetsa2\n",
      "Attribute 0: Statistical Area Level 2 Code\n",
      "Attribute 1: Statistical Area Level 2 Name\n",
      "Attribute 2: Total Population\n",
      "Attribute 3: Country of Origin - Vietnam - Percentage\n",
      "Attribute 4: Country of Origin - Lebanon - Count\n",
      "Attribute 5: Country of Origin - China - Percentage\n",
      "Attribute 6: Country of Origin - Germany - Count\n",
      "Attribute 7: Country of Origin - Greece - Percentage\n",
      "Attribute 8: Country of Origin - Germany - Percentage\n",
      "Attribute 9: Country of Origin - Greece - Count\n",
      "Attribute 10: Country of Origin - India - Count\n",
      "Attribute 11: Country of Origin - India - Percentage\n",
      "Attribute 12: Country of Origin - Italy - Count\n",
      "Attribute 13: Country of Origin - Italy - Percentage\n",
      "Attribute 14: Country of Origin - Lebanon - Percentage\n",
      "Attribute 15: Country of Origin - Malaysia - Count\n",
      "Attribute 16: Country of Origin - Philippines - Percentage\n",
      "Attribute 17: Country of Origin - Malaysia - Percentage\n",
      "Attribute 18: Country of Origin - Philippines - Count\n",
      "Attribute 19: Country of Origin - Sri Lanka - Count\n",
      "Attribute 20: Country of Origin - Sri Lanka - Percentage\n",
      "Attribute 21: Country of Origin - China - Count\n",
      "Attribute 22: Country of Origin - Vietnam - Count\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    729\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 730\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    731\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    795\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 796\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    797\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \"\"\"\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:7683)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:7460)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy (zmq/backend/cython/socket.c:2344)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc (zmq/backend/cython/socket.c:9621)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-444bbca3f0f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mselect\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPORTAL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeo_classification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeo_properties\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maurin_props\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchooseAttributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-8-e530d98a42b8>\u001b[0m in \u001b[0;36mchooseAttributes\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;31m# select attributes for analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mselected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselectAttributes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;31m#The user wants all attributes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-c6cb4aad2c07>\u001b[0m in \u001b[0;36mselectAttributes\u001b[0;34m(folder)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;31m#allow the user to select by index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0muser_select\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Enter the attribute number you would like to use: (For all attributes, leave blank)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0muser_select\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 735\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    736\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#allow the user to choose the first dataset\n",
    "if select == PORTAL:\n",
    "    \n",
    "    data, metadata, geo_classification, geo_properties, aurin_props = chooseAttributes()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Datasets from the API\n",
    "\n",
    "> Note: This section of code has been adapted from Qi (2017).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Requesting to get the capabilities of the metadata service in AURIN.\n",
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "import configparser\n",
    "import urllib.request, urllib.error, urllib.parse\n",
    "from collections import defaultdict\n",
    "from dbfread import DBF\n",
    "from lxml import etree\n",
    "\n",
    "# Import authentication parameters from the config file\n",
    "config = configparser.RawConfigParser()\n",
    "config.read('openapi.cfg')\n",
    "\n",
    "username=config.get('Auth', 'username')\n",
    "password=config.get('Auth', 'password')\n",
    "\n",
    "# Submit an authenticated request to the AURIN Open API\n",
    "def openapi_request(url):\n",
    "\n",
    "    # create an authenticated HTTP handler and submit URL\n",
    "    password_manager = urllib.request.HTTPPasswordMgrWithDefaultRealm()\n",
    "    password_manager.add_password(None, url, username, password)\n",
    "    auth_manager = urllib.request.HTTPBasicAuthHandler(password_manager)\n",
    "    opener = urllib.request.build_opener(auth_manager)\n",
    "    urllib.request.install_opener(opener)\n",
    "    \n",
    "    #user_agent = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64)'\n",
    "    #headers = { 'User-Agent' : user_agent }\n",
    "    req = urllib.request.Request(url)\n",
    "    try:\n",
    "        handler = urllib.request.urlopen(req)\n",
    "        return handler.read()\n",
    "    except urllib.error.HTTPError as err:\n",
    "        if err.code == 404:\n",
    "            #When requesting the metadata by url, the server limits the successful times, so when\n",
    "            #meeting HTTPError 404, print the message and request again.\n",
    "            print ('Trying to access with url...')\n",
    "            return openapi_request(url)\n",
    "        else:\n",
    "            raise\n",
    "        \n",
    "        \n",
    "# Get the capabilities of the metadata service\n",
    "url = 'http://openapi.aurin.org.au/csw?request=GetCapabilities&service=CSW'\n",
    "xml = openapi_request(url)\n",
    "root = etree.fromstring(xml)\n",
    "print (\"------------------------------------------------------------------------\")\n",
    "print (\"Get capabilities successfully.\")\n",
    "print (\"------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##### Zoe's Code --> Used to search by topic\n",
    "def get_all_datasets():\n",
    "    dataset_names = []#This is a list of dataset names we can use to access its features\n",
    "\n",
    "    # Get a list of all available datasets and their licences\n",
    "    url='http://openapi.aurin.org.au/csw?request=GetRecords&service=CSW&version=2.0.2&typeNames=csw:Record&elementSetName=full&resultType=results&constraintLanguage=CQL_TEXT&constraint_language_version=1.1.0&maxRecords=1000'\n",
    "    xml = openapi_request(url)\n",
    "    root = etree.fromstring(xml)\n",
    "     \n",
    "    subs = set()#save all subjects available to choose\n",
    "    for dataset in root.findall(\".//csw:Record\", root.nsmap):\n",
    "        for subject in dataset.findall(\".//dc:subject\",root.nsmap):\n",
    "            if not subject.text.startswith('ANZ'):\n",
    "                subs.add(subject.text)\n",
    "    print ('Please choose subject you are interested in from:')\n",
    "    print (subs)\n",
    "    target_sub = input('I want to search:')\n",
    "     \n",
    "    for dataset in root.findall(\".//csw:Record\", root.nsmap):\n",
    "        #Start searching relavant datasets\n",
    "        flag = 0\n",
    "        for subject in dataset.findall(\".//dc:subject\",root.nsmap):\n",
    "            if subject.text == target_sub:\n",
    "                flag = 1\n",
    "                break\n",
    "        #Printing\n",
    "        if flag == 1:\n",
    "            print('================ DATASET + SUBJECTS ================')\n",
    "            name = dataset.find(\".//dc:identifier\",root.nsmap).text\n",
    "            dataset_names.append(name)\n",
    "            print (name)\n",
    "            for subject in dataset.findall(\".//dc:subject\",root.nsmap):\n",
    "                print (subject.text) \n",
    "                \n",
    "    return dataset_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getFeatures(dataset):\n",
    "    url = 'http://openapi.aurin.org.au/wfs?request=DescribeFeatureType&service=WFS&version=1.1.0&typeName='+name\n",
    "    \n",
    "    xml = openapi_request(url)\n",
    "    root = etree.fromstring(xml)\n",
    "    property_dict = defaultdict(list)\n",
    "\n",
    "    # find the aggregation level of the dataset, if any\n",
    "    keylist, geographical_classifications = (findAggr(root, True))\n",
    "    \n",
    "    for element in root.find(\".//xsd:sequence\", root.nsmap):\n",
    "        property_name = element.get('name')\n",
    "        property_dict[property_name] = None\n",
    "    \n",
    "    \n",
    "    if keylist == []:\n",
    "        raise Exception('Invalid dataset. No valid aggregation level feature is included.')\n",
    "        \n",
    "        # the user has to manually tell the program what geographical property is the aggregation (if any) \n",
    "        # and also declaring the classification of the property\n",
    "        for element in root.find(\".//xsd:sequence\", root.nsmap):\n",
    "            property_name = element.get('name')\n",
    "            print (property_name)\n",
    "        geo_properties = input(\"Please manually enter the geographical property of the dataset\")\n",
    "        geo_classifications = input(\"Please manually enter the geographical classification of the dataset\")\n",
    "        keylist = [geo_properties]\n",
    "        geo_classifications = [geo_classifications]\n",
    "        \n",
    "    print (\"The dataset has the following geographical classifications: \", geographical_classifications)\n",
    "    \n",
    "    \n",
    "    # This is relevant is the dataset has multiple geographical classifications\n",
    "    key_index = input('Please input a valid aggregation/geogrpahical classification level code (Index): ')\n",
    "    chosen_classification = geographical_classifications[int(key_index)]\n",
    "    \n",
    "    return keylist, geographical_classifications, chosen_classification, property_dict\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Load the data of the particular dataset from the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def selectFeatures():\n",
    "    \n",
    "    print ('Total features we have now are:')\n",
    "    \n",
    "    counter = 0\n",
    "    for name in prop_list:\n",
    "        print (counter, name)\n",
    "        counter += 1\n",
    "    \n",
    "    selected = input(\"I select:\") \n",
    "    #sa2_main11 or postcode can be seen as the key item in choose, so user does not need to add it. \n",
    "    choose = list(prop_list)[int(selected)]\n",
    "    \n",
    "              \n",
    "    return choose\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Given a feature, getFeatureValue will return all values through the API\n",
    "def getFeatureValue(prop):\n",
    "    #This can get all values for property_name\n",
    "    url = 'http://openapi.aurin.org.au/wfs?request=GetPropertyValue&service=WFS&version=2.0.0&TypeName='+name+'&valueReference='+prop\n",
    "    \n",
    "    xml = openapi_request(url)\n",
    "    root = etree.fromstring(xml)\n",
    "\n",
    "    values = []\n",
    "    for member in root.findall('.//wfs:member',root.nsmap):\n",
    "        value = member.find('.//aurin:'+prop,root.nsmap).text\n",
    "        values.append(value) \n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The code will run from here if the user selects obtaining data from the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if select == API:\n",
    "    dataset_names = get_all_datasets()\n",
    "    \n",
    "    \n",
    "    #Let user choose dataset.\n",
    "    if dataset_names == []:\n",
    "        raise Exception('No valid dataset is selected.')\n",
    "    else:\n",
    "        counter = 0\n",
    "        for name in dataset_names:\n",
    "            print(counter, name)\n",
    "            counter += 1\n",
    "\n",
    "        i = input('Please select the index that you want:')\n",
    "        name = dataset_names[int(i)]\n",
    "        print ('You have selected:',name)\n",
    "    \n",
    "    # Allow the User to select datasets by subject\n",
    "    geo_properties, geo_classification, chosen_classification, property_dict = getFeatures(name)\n",
    "    #choose: saving all selected features and the keyname(the feature which specifies aggregation level)\n",
    "    \n",
    "    \n",
    "    prop_list = property_dict.keys()\n",
    "    \n",
    "    \n",
    "    choose = selectFeatures()\n",
    "    print ('Selected features are:', choose)\n",
    "\n",
    "    data = defaultdict(list)\n",
    "    aurin_props = [{\"name\": choose}]\n",
    "    geo_classification = geo_classification[0]\n",
    "    \n",
    "    for prop in geo_properties:\n",
    "        data[prop] = getFeatureValue(prop)\n",
    "    \n",
    "    data[choose] = getFeatureValue(prop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# We now load the data from ADDN/ENDIA/Type 2 Diabetes Data\n",
    "\n",
    "Allow the user to choose what data they want to load\n",
    "\n",
    "> Note: This can be easily changed to load new sets of data, e.g. Datasets on Astham, Cystic Fibrosis etc.\n",
    "\n",
    "> **In tis project, there were 3 datasets analysed**\n",
    "> - ENDIA Dataset (Type 1 Diabetes)\n",
    "> - ADDN Dataset (Type 1 Diabetes)\n",
    "> - PHIDU Dataset (Diabetes in general)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENDIA = \"0\"\n",
    "ADDN = \"1\"\n",
    "T2D = \"2\"\n",
    "\n",
    "select = input(\"Which database do you want to load patient data from? ENDIA(0) ADDN(1) PHIDU-T2D(2)\")\n",
    "print(\"You selected: \", select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#patient_dict is a dictionary which keeps keyname as key and patient_num as value.\n",
    "import csv\n",
    "patient_dict = defaultdict(int)\n",
    "\n",
    "diabetes_geo = \"\"\n",
    "patient_type = \"current\"\n",
    "\n",
    "\n",
    "######\n",
    "# Load the diabetes data according to user selections\n",
    "######\n",
    "#Collect patient distribution at SA2 level.\n",
    "if select == ENDIA:\n",
    "    diabetes_geo = \"sa2\"\n",
    "    for record in DBF('endia-sa2/04eba726-cea8-4ba6-8911-0be82949c851.dbf'):\n",
    "        patient_dict[str(record['SA2_MAIN11'])] = int(record['count'])\n",
    "    \n",
    "    \n",
    "#Collect patient distribution at postcode level.   \n",
    "elif select == ADDN:\n",
    "    diabetes_geo = \"postcode\"\n",
    "    print ('Please choose 1(diagnosed postcode); 2(current postcode)')\n",
    "    index = input('I select')\n",
    "    index = int(index)\n",
    "    if index != 1 and index != 2:\n",
    "        raise Exception('Input error!')\n",
    "    if index ==1:\n",
    "        patient_type = \"diagnosed\"\n",
    "    with open('ADDN-Postcode-Data_2017-09-12.csv', 'r') as infile:\n",
    "        r = csv.reader(infile)\n",
    "        next(r)\n",
    "        for row in r: \n",
    "            patient_dict[str(row[0])] = int(row[index])\n",
    "elif select == T2D:\n",
    "    diabetes_geo = \"sa2\"\n",
    "    with open('type2diabetes.csv', 'r') as infile:\n",
    "        patient_type = \"predicted\"\n",
    "        r = csv.reader(infile)\n",
    "        next(r)\n",
    "        for row in r: \n",
    "            try:\n",
    "                index = int(row[0])\n",
    "                patient_dict[str(index)] = float(row[1])\n",
    "            except ValueError:\n",
    "                continue\n",
    "    \n",
    "print (\"------------------------------------------------------------------------\")\n",
    "print ('Diabetes Data successfully loaded.')\n",
    "print (\"------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With the two datasets loaded, we now convert between them two using correspondances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the data into the higher ranked aggregation.\n",
    "\n",
    "\n",
    "The Ranking of the Aggregation Levels are:\n",
    "\n",
    "> sa1 -> postcode -> sla -> sa2 -> lga -> sa3 -> sa4  \n",
    "\n",
    "Higher ranked aggregation levels cover larger geographical areas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Define the ranking of the different correspondances\n",
    "def determineHigherAggr(aggr1, aggr2):\n",
    "    \"\"\"This function takes two aggregation levels and returns the higher aggregation level\n",
    "    It will return False if the two aggregation levels are the same\n",
    "\n",
    "    \"\"\"\n",
    "    if aggr1 == aggr2:\n",
    "        return False\n",
    "    \n",
    "\n",
    "    \n",
    "    ranking_list = [\"sa1\", \"postcode\", \"sla\", \"sa2\", \"lga\", \"sa3\", \"sa4\"]\n",
    "    \n",
    "    \n",
    "    \n",
    "    index1 = ranking_list.index(aggr1)\n",
    "    index2 = ranking_list.index(aggr2)\n",
    "    \n",
    "    \n",
    "    # return the higher ranked aggregation\n",
    "    if index1 < index2:\n",
    "        return aggr2\n",
    "    else:\n",
    "        return aggr1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadCorrespondanceFolder(aggr_level_from, aggr_level_to):\n",
    "    \"\"\"This function returns the name of the relevant folder that we need to open a correspondance file\"\"\"\n",
    "    \n",
    "    correspondance_folder = \"\"\n",
    "    correspondance_file = \"\"\n",
    "    \n",
    "    for (dirpath, dirnames, filenames) in os.walk(\"./Correspondances/\"):\n",
    "        for name in dirnames:\n",
    "            # first check if it's possible convert between the two aggr levels\n",
    "            # checking if there is a correspondence file being able to convert between the two aggr levels\n",
    "            if aggr_level_from in name.lower() and aggr_level_to in name.lower():\n",
    "                correspondance_folder = name\n",
    "                \n",
    "                # split the file name\n",
    "                mix_case = correspondance_folder.split(\"_\")\n",
    "                \n",
    "                filename_list =[i.lower() for i in mix_case]\n",
    "\n",
    "                # the correspondence file has the pattern of the aggregation it's converting from is alway listed in the title\n",
    "                # before the aggregation level it's converting to\n",
    "                index_from = filename_list.index(aggr_level_from)\n",
    "                index_to = filename_list.index(aggr_level_to)\n",
    "\n",
    "                \n",
    "                if index_from > index_to:\n",
    "                    continue\n",
    "                else:\n",
    "                    break\n",
    "    \n",
    "    \n",
    "    return correspondance_folder\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import xlrd\n",
    "\n",
    "def getCorrespondances(aggr_level_from, aggr_level_to):\n",
    "    \"\"\"This function will return a dataset in the form of a dictionary\n",
    "    of the amount of correspondance between two aggregation levels\"\"\"\n",
    "    correspondance_folder = loadCorrespondanceFolder(aggr_level_from, aggr_level_to)\n",
    "    print(\"Correspondance_folder is:\", correspondance_folder)\n",
    "    correspondance_file = \"./Correspondances/\" + correspondance_folder + \"/\"\n",
    "    \n",
    "    # find the excel file\n",
    "    if correspondance_folder is not None:\n",
    "        for (dirpath, dirnames, filenames) in os.walk(\"./Correspondances/\" + correspondance_folder):\n",
    "            correspondance_file += filenames[0]\n",
    "            \n",
    "            \n",
    "    # e.g. [[the aggr code from],[the aggr code to],[the correspondance ratio]]\n",
    "    values_to = [[],[],[]]\n",
    "    \n",
    "    # Load the excel file\n",
    "    wb = xlrd.open_workbook(correspondance_file)\n",
    "    \n",
    "    \n",
    "    # Most of the time the correspondance values as in Table 3\n",
    "    # Load the right spreadsheet that we want\n",
    "    sheet = wb.sheet_by_name('Table 3')\n",
    "    \n",
    "    max_row = sheet.nrows\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # the Aggr_level converting from is usually in Column B\n",
    "    aggr_from_column = 0\n",
    "    \n",
    "    # the Aggr_level converting to is usually in Column C\n",
    "    aggr_to_column = 2\n",
    "    # The ratio of the match is on column E\n",
    "    \n",
    "    ratio_column = 4\n",
    "    \n",
    "    # All of the values start at 8\n",
    "    starting_row = 7\n",
    "    \n",
    "    # Get the relevant values and load them into values_to dict\n",
    "    for i in range(starting_row, max_row):\n",
    "        aggr_from = sheet.cell(i,aggr_from_column).value\n",
    "        aggr_to = sheet.cell(i,aggr_to_column).value\n",
    "        aggr_ratio = sheet.cell(i, ratio_column).value\n",
    "        try:\n",
    "            values_to[0].append(int(aggr_from))\n",
    "            values_to[1].append(int(aggr_to))\n",
    "            values_to[2].append(aggr_ratio)\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return values_to\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#This is the main function which will convert data from one geographic aggregation into another\n",
    "\n",
    "def convertAggr(aurin_data, aurin_geo_class, aurin_geo_prop, aurin_prop, diabetes_data, diabetes_geo_class, diabetes_data_type):\n",
    "    \"\"\"This function takes two different datasets, one from ADDN/ENDIA, the other loaded from AURIN,\n",
    "    It will use the correspondances obtained from the ABS of the different aggregation levels\n",
    "    And it will convert the two datasets into a single aggregation level\"\"\"\n",
    "    \n",
    "    #Determine which is the higher aggr\n",
    "    higher_aggr = determineHigherAggr(aurin_geo_class, diabetes_geo_class)\n",
    "    print(\"The higher aggregation we will covert to is: \", higher_aggr)\n",
    "    \n",
    "    # e.g. {\"5072\": {\"sendentric behaviour\": 9}}\n",
    "    combined_data = defaultdict(dict)\n",
    "    \n",
    "    # The two dataset have the same aggregation\n",
    "    if higher_aggr == False:\n",
    "        higher_aggr = diabetes_geo_class\n",
    "\n",
    "    \n",
    "    #Load the data of the higher aggr into the ret_dict\n",
    "    \n",
    "    \n",
    "    if higher_aggr == aurin_geo_class:\n",
    "        \n",
    "        # iterate through the higher_aggr dataset and load every area into the combined_data\n",
    "        for i in range(0,len(aurin_data[aurin_geo_prop])):\n",
    "            \n",
    "            combined_data[aurin_data[aurin_geo_prop][i]][aurin_prop] = aurin_data[aurin_prop][i]\n",
    "        \n",
    "        # get the correspondances\n",
    "        correspondances = getCorrespondances(diabetes_geo_class, aurin_geo_class)\n",
    "        \n",
    "        # for each aggr, determine the higher aggrs that it corresponds with \n",
    "        for i in range(0, len(correspondances[0])):\n",
    "            \n",
    "            aggr_from_code = str(correspondances[0][i])\n",
    "            aggr_to_code = str(correspondances[1][i])\n",
    "            try:\n",
    "                ratio = float(correspondances[2][i])\n",
    "            # if there's no ratio, to prevent an error a 0 is automatically assigned\n",
    "            except:\n",
    "                ratio = 0\n",
    "                \n",
    "              \n",
    "            # the patient values adding in start at 0\n",
    "            # proportions of patients will be added to this according to the correspondences\n",
    "            combined_data[aggr_to_code][diabetes_data_type] = 0.0\n",
    "            \n",
    "            # read through diabetes data looking for matches\n",
    "            # load proportions of that data into corresponding higher aggr\n",
    "            try:\n",
    "                no_patients = diabetes_data[aggr_from_code]\n",
    "                combined_data[aggr_to_code][diabetes_data_type] += float(no_patients)*ratio\n",
    "            except KeyError:\n",
    "                print(\"Couldn't Find key\")\n",
    "           \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "           \n",
    "            \n",
    "            \n",
    "    # perform the same thing except this time with the diabetes data as the higher aggregation\n",
    "    elif higher_aggr == diabetes_geo_class:\n",
    "        global geo_classification \n",
    "        geo_classification = diabetes_geo_class\n",
    "        temp_aurin_data = defaultdict(dict)\n",
    "        \n",
    "        for key, value in diabetes_data.items():\n",
    "            combined_data[key][diabetes_data_type] = float(value)\n",
    "            \n",
    "        for i in range(0,len(aurin_data[aurin_geo_prop])):\n",
    "            \n",
    "            temp_aurin_data[aurin_data[aurin_geo_prop][i]][aurin_prop] = aurin_data[aurin_prop][i]\n",
    "    \n",
    "        correspondances = getCorrespondances(aurin_geo_class, diabetes_geo_class)\n",
    "        \n",
    "        \n",
    "        for i in range(0, len(correspondances[0])):\n",
    "            \n",
    "            aggr_from_code = str(correspondances[0][i])\n",
    "            aggr_to_code = str(correspondances[1][i])\n",
    "            try:\n",
    "                ratio = float(correspondances[2][i])\n",
    "            except:\n",
    "                print(\"The ratio is: \", correspondances[2][i])\n",
    "                ratio = 0\n",
    "                \n",
    "                \n",
    "            \n",
    "            combined_data[aggr_to_code][aurin_prop] = 0.0\n",
    "            \n",
    "            #read through diabetes data looking for matches\n",
    "            try:\n",
    "                prop_value = temp_aurin_data[aggr_from_code][aurin_prop]\n",
    "                combined_data[aggr_to_code][aurin_prop] += float(prop_value)*ratio\n",
    "            except KeyError:\n",
    "                print(\"Couldn't Find key\")\n",
    "            except ValueError:\n",
    "                print(\"Can't covert the value to a float\")\n",
    "    \n",
    "            \n",
    "    return combined_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if geo_classification != diabetes_geo:\n",
    "    combined_data = convertAggr(data, geo_classification, geo_properties[0], aurin_props[0][\"name\"], patient_dict, diabetes_geo, patient_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(geo_classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using shape files, get the shapes required for the particular aggregation level\n",
    "\n",
    "> Note: Some of this section was adapted from the work of Qi (2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Collecting the geometric data\n",
    "\n",
    "import shapefile\n",
    "from geojson import Polygon\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------\n",
    "# Partially Derived from Qi (2017)\n",
    "#---------------------------------------------------------------------\n",
    "def getGeometry(filename):\n",
    "    \"\"\" This function will return a dictionary outlining the coordinates of the polygon based on a shapefile input \"\"\"\n",
    "    poly_dict = defaultdict(Polygon)\n",
    "    \n",
    "    sf = shapefile.Reader(filename)\n",
    "\n",
    "    #395 in total\n",
    "    counter = 0\n",
    "    for shape in sf.shapeRecords():\n",
    "        poly = []\n",
    "        key = shape.record[1]\n",
    " \n",
    "        for longi,lati in shape.shape.points:\n",
    "            poly.append([longi, lati])\n",
    "       \n",
    "            counter +=1\n",
    "            \n",
    "        poly = Polygon([poly])\n",
    "        poly_dict[str(key)]=poly    \n",
    "    return poly_dict\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the relevant shapefile according to the geographical aggregation\n",
    "\n",
    "if geo_classification == \"lga\":\n",
    "    polygon_dict = getGeometry('./Shape_Files/LGA11aAust.shp')\n",
    "elif geo_classification == \"sa1\":\n",
    "    polygon_dict = getGeometry('./Shape_Files/1270055001_sa1_2016_aust_shape/SA1_2016_AUST.shp')\n",
    "elif geo_classification == \"sa2\":\n",
    "    polygon_dict = getGeometry('./endia-sa2/04eba726-cea8-4ba6-8911-0be82949c851.shp')\n",
    "elif geo_classification == \"sa3\":\n",
    "    polygon_dict = getGeometry('./Shape_Files/1270055001_sa3_2016_aust_shape/SA3_2016_AUST.shp')\n",
    "elif geo_classification == \"sa4\":\n",
    "    polygon_dict = getGeometry('./Shape_Files/1270055001_sa4_2016_aust_shape/SA4_2016_AUST.shp')\n",
    "elif geo_classification == \"sla\":\n",
    "    print(\"SLA Loaded\")\n",
    "    polygon_dict = getGeometry('./Shape_Files/SLA11aAust.shp')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def addGeometry(data, polygon_dict, geo_name, prop, patient_type):\n",
    "    \"\"\"This function adds the geometry polygons to the dataset, allowing it to be visualised \n",
    "    Essentially combining the polygon dict with the dataset\"\"\"\n",
    "    \n",
    "    combined = []\n",
    "    feature_list = []\n",
    "    for key, value in polygon_dict.items():\n",
    "\n",
    "        try:\n",
    "            combined.append({geo_name: key, 'geometry': value, prop: data[key][prop]})\n",
    "            feature_list.append({'geometry': value, \"properties\": {prop: data[key][prop], geo_name: key}, \"type\": \"Feature\"})\n",
    "            combined[-1][\"count\"] = data[key][patient_type]\n",
    "            feature_list[-1][\"properties\"][\"count\"] = data[key][patient_type]\n",
    "        \n",
    "        # skip any areas without a property value as these areas should be eliminated from analysis    \n",
    "        except KeyError:\n",
    "            continue\n",
    "          \n",
    "\n",
    "\n",
    "    return combined, feature_list\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Allow the user to input whether they want to eliminate the regions with low patient numbers from the analysis\n",
    "\n",
    "> - Type 1 Diabetes data in particular lacks in sample size. Hence it's likely that a lot of aggregated areas does not have any patients. While this may be a true fact in that these areas indeed don't have any diabetes patients, including a significant amount of 0 values in the analysis is likely to affect correlations and statistical measurements\n",
    "> - Also, as we are analysing trends with numerical variables, 0 values are unlikely to be of use as they don't tend to show a relationship between the two variable instead simply clusters the graphs up with unnecessary points\n",
    ">> **Thus the user has the option of choosing a minimum patient value for each area/region**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_patient=input(\"Please input the minimum number of patients per region (put 0 to include all data)\")\n",
    "print(\"We will not include regions with less than {} patients\".format(minimum_patient))\n",
    "print(\"====================================================================================\")\n",
    "minimum_prop=input(\"Please input the minimum property value per region (put 0 to include all data)\")\n",
    "print(\"We will not include regions with less than {} as its property value\".format(minimum_prop))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# delete the areas with patient numbers too small for analysis\n",
    "\n",
    "to_delete = []\n",
    "\n",
    "for k, v in combined_data.items():\n",
    "    try:\n",
    "        combined_data[k][aurin_props[0][\"name\"]]\n",
    "        combined_data[k][patient_type]\n",
    "\n",
    "        if combined_data[k][patient_type] < float(minimum_patient):\n",
    "            to_delete.append(k)\n",
    "        elif float(combined_data[k][aurin_props[0][\"name\"]]) < float(minimum_prop):\n",
    "            to_delete.append(k)\n",
    "        \n",
    "        \n",
    "    except KeyError:\n",
    "        to_delete.append(k)\n",
    "        \n",
    "    \n",
    "\n",
    "for i in to_delete:\n",
    "    del combined_data[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "instances, feature_list = addGeometry(combined_data,  polygon_dict,geo_properties[0],aurin_props[0][\"name\"], patient_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "choose = geo_properties + [aurin_props[0][\"name\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform statistical tests on the data\n",
    "\n",
    "> ** Note: This section was adapted from Qi (2017) **\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#x records number of patients\n",
    "x = []\n",
    "for ins in instances:\n",
    "    #print(x)\n",
    "    x.append(ins['count'])\n",
    "\n",
    "#countable feature values\n",
    "total_ins = len(instances)\n",
    "ys = defaultdict(list) # save feature name and their values\n",
    "for feature in choose:\n",
    "    y = []\n",
    "    for instance in instances:\n",
    "        try:\n",
    "            value = float(instance[feature])\n",
    "            y.append(value)\n",
    "        except ValueError:\n",
    "            value = instance[feature].split(',')\n",
    "            y.append(len(value))\n",
    "    if len(y) == total_ins:\n",
    "        ys[feature] = y\n",
    "print (\"------------------------------------------------------------------------\")\n",
    "print ('Countable values successfully collected.')\n",
    "print (\"------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Fit and summarize OLS model\n",
    "mod = sm.OLS(x, y)\n",
    "\n",
    "res = mod.fit()\n",
    "\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pierson Correlations and Significance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "print(\"The Pierson Correlation Coefficient is: \")\n",
    "numpy.corrcoef(x, y)[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum information analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from minepy import MINE\n",
    "\n",
    "m = MINE()\n",
    "\n",
    "for k,val in ys.items(): \n",
    "    #Do not regard aggregation level codes as a countable feature.\n",
    "    if k==geo_properties[0]:\n",
    "        #print(\"continued\")\n",
    "        continue\n",
    "    m.compute_score(x,val)\n",
    "    print('Maximal information coefficient between T1D patient number and %s is %f'% (k,m.mic()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot of the Data\n",
    "\n",
    "This will produce a scatter plot showing the data, comparing the number of patients with the factor that the user has selected.\n",
    "\n",
    "The plot will be saved as an imagefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bokeh.io import show, output_notebook, export_png\n",
    "from bokeh.models import GeoJSONDataSource, ColumnDataSource, HoverTool, LinearColorMapper, LogColorMapper\n",
    "from bokeh.palettes import Purples6 as palettex\n",
    "from bokeh.plotting import figure, save\n",
    "\n",
    "def drawLine(x,y,featurename, geo):\n",
    "    # create a new plot with default tools, using figure\n",
    "    title = \"Correlation between number of patients and \"+aurin_props[0][\"name\"]\n",
    "    \n",
    "    source = ColumnDataSource(data=dict(\n",
    "    x=x,\n",
    "    y=y,\n",
    "    geo=geo\n",
    "    ))\n",
    "    \n",
    "    hover = HoverTool(tooltips=[\n",
    "        (geo_properties[0],'@geo'),\n",
    "        (\"Number of patients,\", \"$x\"),\n",
    "        (aurin_props[0][\"name\"], \"$y\")\n",
    "    ])\n",
    "    \n",
    "    p = figure(title = title,plot_width=800, plot_height=400, tools=[hover], x_axis_label=\"Number of patients\", y_axis_label=aurin_props[0][\"name\"])\n",
    "    # add a circle renderer with a size, color, and alpha\n",
    "    #p.line(x, y, line_width=2)\n",
    "    p.circle(\"x\", \"y\", line_color=\"navy\", fill_color=\"blue\", size=5, source=source)\n",
    "    #\n",
    "    # show the results\n",
    "    output_notebook()\n",
    "    show(p)\n",
    "    \n",
    "    export_png(p, filename=\"./Type2/\"+featurename+ \".png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for k,val in ys.items(): \n",
    "    #Do not regard aggregation level codes as a countable feature.\n",
    "    if k==geo_properties[0]:\n",
    "        #print(\"continued\")\n",
    "        continue\n",
    "    drawLine(x,val,k, ys[geo_properties[0]])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the visualisation of the data, the user should select whether they are interested in a particular are/region\n",
    "\n",
    "> These regions maybe those which have higher values (either patients or the factor values), or may be because they might be potential outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "location_interested = input(\"Is there a particular area you're interested in? Y/N\")\n",
    "if location_interested == \"Y\" or location_interested == \"y\":\n",
    "    location_code = input(\"What is the code of the location you're interested in? \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation of a particular area\n",
    "\n",
    "The user is able to input a particular area of interest which they would like to view on a map\n",
    "\n",
    "By seeing the region on Google Maps, they are able to determine whether there are specific geographical factors which may influence the findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list_to_map = []\n",
    "\n",
    "for i in range(len(feature_list)):\n",
    "    if feature_list[i][\"properties\"][geo_properties[0]] == location_code:\n",
    "        print(\"The location has been found!!\")\n",
    "        \n",
    "        try:\n",
    "            feature_list_to_map.append(feature_list[i])\n",
    "\n",
    "        except IndexError:\n",
    "            print(\"Index Error\")\n",
    "        break\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualise the data on a map\n",
    "import geojson\n",
    "\n",
    "geo_shape = 1\n",
    "fc = geojson.FeatureCollection(feature_list_to_map)\n",
    "geojsonData = geojson.dumps(fc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.io import output_file, show\n",
    "from bokeh.models import GeoJSONDataSource, ColumnDataSource, HoverTool, LinearColorMapper, LogColorMapper\n",
    "from bokeh.palettes import Viridis3 as palettex\n",
    "from bokeh.models import (\n",
    "  GMapPlot, GMapOptions, ColumnDataSource, Circle, DataRange1d, PanTool, WheelZoomTool, BoxSelectTool, Patches\n",
    ")\n",
    "from shapely.geometry import shape, Polygon\n",
    "import json\n",
    "\n",
    "j = (json.loads(geojsonData))\n",
    "polygon = shape(j[\"features\"][0][\"geometry\"])\n",
    "\n",
    "point = polygon.centroid\n",
    "\n",
    "geo_source = GeoJSONDataSource(geojson=geojsonData)\n",
    "palettex = palettex[::-1]\n",
    "color_mapper = LogColorMapper(palette=palettex)\n",
    "\n",
    "map_options = GMapOptions(lat=point.y, lng=point.x, map_type=\"roadmap\", zoom=11)\n",
    "\n",
    "plot = GMapPlot(\n",
    "    x_range=DataRange1d(), y_range=DataRange1d(), map_options=map_options\n",
    ")\n",
    "plot.title.text = location_code + \" plotted on Google Maps\"\n",
    "\n",
    "\n",
    "plot.api_key = \"AIzaSyAcSa5p9ZWD7mRB2RMpu-NdsXXrCvt4kHU\"\n",
    "\n",
    "\n",
    "patches = Patches(xs='xs',ys='ys',fill_alpha=.1, fill_color={'field': 'count', 'transform': color_mapper}, line_color=\"#440154\", line_width=0.1, line_alpha=0.8)\n",
    "plot.add_glyph(geo_source, patches)\n",
    "\n",
    "display_prop = [(geo_properties[0],location_code),\n",
    "        (\"Number of patients,\", str(feature_list_to_map[0][\"properties\"][\"count\"])),\n",
    "        (aurin_props[0][\"name\"], feature_list_to_map[0][\"properties\"][aurin_props[0][\"name\"]])]\n",
    "\n",
    "\n",
    "\n",
    "plot.add_tools(PanTool(), WheelZoomTool(), BoxSelectTool(), HoverTool(tooltips=display_prop))\n",
    "\n",
    "output_notebook()\n",
    "show(plot)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
